{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Problem\n",
    "use a hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Hidden Layer && Single Classification\n",
    "+ Input layer $\\bf{x} \\in \\mathbb{R}^n$\n",
    "+ Hidden layer $\\bf{W_1} \\in \\mathbb{R}^{m \\times n},\\bf{b_1} \\in \\mathbb{R}^m$\n",
    "+ Output layer $\\bf{w_2} \\in \\mathbb{R}^{m}, \\it{b_2} \\in \\mathbb{R}$\n",
    "\n",
    "$$\\bf{h} = \\sigma (\\bf{W_1x} + \\bf{b_1})$$\n",
    "$$\\it{o} = \\bf{w_2^Th} + \\it{b_2}$$\n",
    "\n",
    "Activation function $\\sigma$ should not be a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Activation Function\n",
    "\n",
    "$$\\rm{sigmoid}(\\it{x}) = \\frac{\\rm{1}}{\\rm{1} + exp(-\\it{x})}$$\n",
    "\n",
    "this function is a soft version of $$\\sigma(x) = \\begin{cases} 1  &\\text{\\bf{if} x > 0} \\\\ 0  & \\text{\\bf{otherwise}} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh Activation Function\n",
    "thah activation function project the input to (-1,1).\n",
    "$$tanh(x) = \\frac{\\text{1 - exp(-2x)}}{\\text{1 + exp(-2x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RelU Activation Function\n",
    "ReLU: rectified linear unit.\n",
    "$$$$\n",
    "$$\\rm{ReLU(x) = max(\\it{x} , 0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Classification\n",
    "with a hidden layer added , softmax regression turns to a multi-layer proceptron.\n",
    "+ Input layer $\\bf{x} \\in \\mathbb{R}^n$\n",
    "+ Hidden layer $\\bf{W_1} \\in \\mathbb{R}^{m \\times n},\\bf{b_1} \\in \\mathbb{R}^m$\n",
    "+ Output layer $\\bf{W_2} \\in \\mathbb{R}^{m \\times k}, \\bf{b_2} \\in \\mathbb{R}^k$\n",
    "\n",
    "$$\\bf{h} = \\sigma (\\bf{W_1x} + \\bf{b_1})$$\n",
    "$$\\bf{o} = \\bf{w_2^Th} + \\bf{b_2}$$\n",
    "$$\\bf{y} = softmax(o)$$\n",
    "\n",
    "Activation function $\\sigma$ should not be a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Hidden Layer\n",
    "$$ \\bf{h_1 = \\sigma(W_1x + b_1)} $$\n",
    "$$ \\bf{h_2 = \\sigma(W_2h_1 + b_2)} $$\n",
    "$$ \\bf{h_3 = \\sigma(W_3h_2 + b_3)} $$\n",
    "$$ \\bf{o = W_4h_3 + b_4}$$\n",
    "\n",
    "using multi hidden layer requires two hyper-parameters , the numbers of hidden layer and the size of each hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "+ multi-layer proceptron use hidden layer and activation function to get a nolinear model.\n",
    "+ Sigmoid , Tanh , ReLU are three activation functions.\n",
    "+ using softmax method to handle multi-classification.\n",
    "+ hyper-parameters : number of layers , size of each layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5806b5e37cc811bf044ba5e2988deab9f2c2c503098f64f4090706569776171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
